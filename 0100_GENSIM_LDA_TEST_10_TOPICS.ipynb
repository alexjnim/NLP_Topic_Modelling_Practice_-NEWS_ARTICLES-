{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data, dictionary and lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17283</td>\n",
       "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17284</td>\n",
       "      <td>Rift Between Officers and Residents as Killing...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Benjamin Mueller and Al Baker</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>After the bullet shells get counted, the blood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17285</td>\n",
       "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Margalit Fox</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17286</td>\n",
       "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>William McDonald</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17287</td>\n",
       "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Choe Sang-Hun</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>53287</td>\n",
       "      <td>73465</td>\n",
       "      <td>Rex Tillerson Says Climate Change Is Real, but …</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>Robinson Meyer</td>\n",
       "      <td>2017-01-11</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As chairman and CEO of ExxonMobil, Rex Tillers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>53288</td>\n",
       "      <td>73466</td>\n",
       "      <td>The Biggest Intelligence Questions Raised by t...</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>Amy Zegart</td>\n",
       "      <td>2017-01-11</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I’ve spent nearly 20 years looking at intellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>53289</td>\n",
       "      <td>73467</td>\n",
       "      <td>Trump Announces Plan That Does Little to Resol...</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>Jeremy Venook</td>\n",
       "      <td>2017-01-11</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Donald Trump will not be taking necessary st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>53290</td>\n",
       "      <td>73468</td>\n",
       "      <td>Dozens of For-Profit Colleges Could Soon Close</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>Emily DeRuy</td>\n",
       "      <td>2017-01-11</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dozens of   colleges could be forced to close ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>53291</td>\n",
       "      <td>73469</td>\n",
       "      <td>The Milky Way’s Stolen Stars</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>Marina Koren</td>\n",
       "      <td>2017-01-11</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The force of gravity can be described using a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     id                                              title  \\\n",
       "0               0  17283  House Republicans Fret About Winning Their Hea...   \n",
       "1               1  17284  Rift Between Officers and Residents as Killing...   \n",
       "2               2  17285  Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...   \n",
       "3               3  17286  Among Deaths in 2016, a Heavy Toll in Pop Musi...   \n",
       "4               4  17287  Kim Jong-un Says North Korea Is Preparing to T...   \n",
       "...           ...    ...                                                ...   \n",
       "49995       53287  73465   Rex Tillerson Says Climate Change Is Real, but …   \n",
       "49996       53288  73466  The Biggest Intelligence Questions Raised by t...   \n",
       "49997       53289  73467  Trump Announces Plan That Does Little to Resol...   \n",
       "49998       53290  73468    Dozens of For-Profit Colleges Could Soon Close    \n",
       "49999       53291  73469                       The Milky Way’s Stolen Stars   \n",
       "\n",
       "          publication                         author        date    year  \\\n",
       "0      New York Times                     Carl Hulse  2016-12-31  2016.0   \n",
       "1      New York Times  Benjamin Mueller and Al Baker  2017-06-19  2017.0   \n",
       "2      New York Times                   Margalit Fox  2017-01-06  2017.0   \n",
       "3      New York Times               William McDonald  2017-04-10  2017.0   \n",
       "4      New York Times                  Choe Sang-Hun  2017-01-02  2017.0   \n",
       "...               ...                            ...         ...     ...   \n",
       "49995        Atlantic                 Robinson Meyer  2017-01-11  2017.0   \n",
       "49996        Atlantic                     Amy Zegart  2017-01-11  2017.0   \n",
       "49997        Atlantic                  Jeremy Venook  2017-01-11  2017.0   \n",
       "49998        Atlantic                    Emily DeRuy  2017-01-11  2017.0   \n",
       "49999        Atlantic                   Marina Koren  2017-01-11  2017.0   \n",
       "\n",
       "       month  url                                            content  \n",
       "0       12.0  NaN  WASHINGTON  —   Congressional Republicans have...  \n",
       "1        6.0  NaN  After the bullet shells get counted, the blood...  \n",
       "2        1.0  NaN  When Walt Disney’s “Bambi” opened in 1942, cri...  \n",
       "3        4.0  NaN  Death may be the great equalizer, but it isn’t...  \n",
       "4        1.0  NaN  SEOUL, South Korea  —   North Korea’s leader, ...  \n",
       "...      ...  ...                                                ...  \n",
       "49995    1.0  NaN  As chairman and CEO of ExxonMobil, Rex Tillers...  \n",
       "49996    1.0  NaN  I’ve spent nearly 20 years looking at intellig...  \n",
       "49997    1.0  NaN    Donald Trump will not be taking necessary st...  \n",
       "49998    1.0  NaN  Dozens of   colleges could be forced to close ...  \n",
       "49999    1.0  NaN  The force of gravity can be described using a ...  \n",
       "\n",
       "[50000 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.read_csv('data/articles1.csv')\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"lists/bow_corpus.txt\", \"rb\") as fp:   # Unpickling\n",
    "    bow_corpus = pickle.load(fp)\n",
    "\n",
    "with open(\"lists/norm_corpus_bigrams.txt\", \"rb\") as fp:   # Unpickling\n",
    "    norm_corpus_bigrams = pickle.load(fp)\n",
    "\n",
    "with open(\"lists/norm_papers.txt\", \"rb\") as fp:   # Unpickling\n",
    "    norm_papers = pickle.load(fp)\n",
    "\n",
    "with open(\"lists/pre_papers.txt\", \"rb\") as fp:   # Unpickling\n",
    "    pre_papers = pickle.load(fp)\n",
    "\n",
    "with open(\"lists/pre_titles.txt\", \"rb\") as fp:   # Unpickling\n",
    "    pre_titles = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary.load('models/dictionary.gensim')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENSIM LDA TEST - 10 TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topic_nums' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'topic_nums' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TOTAL_TOPICS = 10\n",
    "lda_model = gensim.models.LdaModel(corpus=bow_corpus, id2word=dictionary,\n",
    "                                   chunksize=1740, alpha=\"auto\", eta=\"auto\",\n",
    "                                   random_state=42, iterations=500, num_topics=TOTAL_TOPICS,\n",
    "                                   passes=20, eval_every=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the LDA model \n",
    "lda_model.save('models/gensim/model_'+str(TOTAL_TOPICS)+'.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "0.036*\"trump\" + 0.020*\"president\" + 0.012*\"obama\" + 0.009*\"would\" + 0.009*\"white_house\" + 0.009*\"russia\" + 0.007*\"country\" + 0.006*\"administration\" + 0.006*\"american\" + 0.006*\"mr\" + 0.006*\"russian\" + 0.005*\"leader\" + 0.005*\"also\" + 0.005*\"united_state\" + 0.005*\"donald_trump\" + 0.004*\"official\" + 0.004*\"former\" + 0.004*\"meeting\" + 0.004*\"government\" + 0.004*\"deal\"\n",
      "\n",
      "Topic #2:\n",
      "0.014*\"police\" + 0.009*\"city\" + 0.008*\"family\" + 0.007*\"two\" + 0.007*\"officer\" + 0.006*\"people\" + 0.006*\"home\" + 0.006*\"day\" + 0.005*\"man\" + 0.005*\"child\" + 0.005*\"according\" + 0.004*\"told\" + 0.004*\"three\" + 0.004*\"shooting\" + 0.004*\"cnn\" + 0.004*\"time\" + 0.004*\"car\" + 0.004*\"state\" + 0.003*\"student\" + 0.003*\"death\"\n",
      "\n",
      "Topic #3:\n",
      "0.047*\"trump\" + 0.021*\"clinton\" + 0.014*\"campaign\" + 0.013*\"republican\" + 0.012*\"donald_trump\" + 0.009*\"say\" + 0.009*\"election\" + 0.009*\"candidate\" + 0.008*\"hillary_clinton\" + 0.008*\"voter\" + 0.007*\"party\" + 0.007*\"people\" + 0.007*\"vote\" + 0.006*\"state\" + 0.006*\"president\" + 0.005*\"cruz\" + 0.005*\"democrat\" + 0.005*\"would\" + 0.005*\"poll\" + 0.005*\"think\"\n",
      "\n",
      "Topic #4:\n",
      "0.011*\"would\" + 0.011*\"state\" + 0.009*\"law\" + 0.007*\"republican\" + 0.006*\"american\" + 0.006*\"bill\" + 0.006*\"people\" + 0.005*\"policy\" + 0.005*\"president\" + 0.004*\"house\" + 0.004*\"right\" + 0.004*\"issue\" + 0.004*\"also\" + 0.004*\"plan\" + 0.004*\"could\" + 0.004*\"senate\" + 0.004*\"government\" + 0.004*\"federal\" + 0.004*\"gop\" + 0.004*\"court\"\n",
      "\n",
      "Topic #5:\n",
      "0.012*\"people\" + 0.012*\"like\" + 0.010*\"get\" + 0.010*\"say\" + 0.008*\"would\" + 0.008*\"think\" + 0.008*\"thing\" + 0.008*\"time\" + 0.008*\"know\" + 0.007*\"going\" + 0.007*\"way\" + 0.006*\"want\" + 0.006*\"go\" + 0.005*\"could\" + 0.005*\"make\" + 0.005*\"even\" + 0.005*\"really\" + 0.005*\"see\" + 0.005*\"back\" + 0.005*\"much\"\n",
      "\n",
      "Topic #6:\n",
      "0.012*\"company\" + 0.007*\"new\" + 0.006*\"china\" + 0.006*\"apple\" + 0.005*\"also\" + 0.005*\"car\" + 0.005*\"tesla\" + 0.005*\"could\" + 0.004*\"google\" + 0.004*\"would\" + 0.004*\"technology\" + 0.004*\"system\" + 0.003*\"facebook\" + 0.003*\"north_korea\" + 0.003*\"ceo\" + 0.003*\"product\" + 0.003*\"use\" + 0.003*\"time\" + 0.003*\"chinese\" + 0.003*\"service\"\n",
      "\n",
      "Topic #7:\n",
      "0.010*\"woman\" + 0.007*\"show\" + 0.005*\"year\" + 0.005*\"also\" + 0.005*\"time\" + 0.004*\"black\" + 0.004*\"life\" + 0.004*\"new\" + 0.003*\"first\" + 0.003*\"like\" + 0.003*\"story\" + 0.003*\"white\" + 0.003*\"film\" + 0.003*\"wrote\" + 0.003*\"star\" + 0.003*\"book\" + 0.003*\"people\" + 0.003*\"movie\" + 0.003*\"made\" + 0.002*\"family\"\n",
      "\n",
      "Topic #8:\n",
      "0.009*\"investigation\" + 0.008*\"report\" + 0.008*\"official\" + 0.007*\"case\" + 0.006*\"information\" + 0.006*\"time\" + 0.006*\"email\" + 0.006*\"according\" + 0.005*\"fbi\" + 0.005*\"also\" + 0.005*\"statement\" + 0.005*\"told\" + 0.004*\"former\" + 0.004*\"comey\" + 0.004*\"evidence\" + 0.004*\"reported\" + 0.004*\"cnn\" + 0.004*\"intelligence\" + 0.004*\"would\" + 0.004*\"charge\"\n",
      "\n",
      "Topic #9:\n",
      "0.017*\"company\" + 0.016*\"year\" + 0.015*\"million\" + 0.010*\"business\" + 0.009*\"market\" + 0.009*\"percent\" + 0.009*\"according\" + 0.008*\"money\" + 0.007*\"price\" + 0.006*\"number\" + 0.006*\"billion\" + 0.006*\"job\" + 0.006*\"also\" + 0.006*\"new\" + 0.006*\"investor\" + 0.005*\"report\" + 0.005*\"cost\" + 0.005*\"study\" + 0.005*\"economy\" + 0.005*\"since\"\n",
      "\n",
      "Topic #10:\n",
      "0.011*\"attack\" + 0.010*\"people\" + 0.010*\"isi\" + 0.010*\"country\" + 0.007*\"government\" + 0.006*\"group\" + 0.006*\"muslim\" + 0.005*\"syria\" + 0.005*\"also\" + 0.004*\"force\" + 0.004*\"killed\" + 0.004*\"military\" + 0.004*\"city\" + 0.004*\"war\" + 0.004*\"many\" + 0.003*\"official\" + 0.003*\"year\" + 0.003*\"security\" + 0.003*\"turkey\" + 0.003*\"two\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic_id, topic in lda_model.print_topics(num_topics=10, num_words=20):\n",
    "    print('Topic #'+str(topic_id+1)+':')\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Coherence Score: -1.3628021129807837\n"
     ]
    }
   ],
   "source": [
    "topics_coherences = lda_model.top_topics(bow_corpus, topn=20)\n",
    "avg_coherence_score = np.mean([item[1] for item in topics_coherences])\n",
    "print('Avg. Coherence Score:', avg_coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics with Weights\n",
      "==================================================\n",
      "Topic #1:\n",
      "[('people', 0.012), ('like', 0.012), ('get', 0.01), ('say', 0.01), ('would', 0.008), ('think', 0.008), ('thing', 0.008), ('time', 0.008), ('know', 0.008), ('going', 0.007), ('way', 0.007), ('want', 0.006), ('go', 0.006), ('could', 0.005), ('make', 0.005), ('even', 0.005), ('really', 0.005), ('see', 0.005), ('back', 0.005), ('much', 0.005)]\n",
      "\n",
      "Topic #2:\n",
      "[('trump', 0.047), ('clinton', 0.021), ('campaign', 0.014), ('republican', 0.013), ('donald_trump', 0.012), ('say', 0.009), ('election', 0.009), ('candidate', 0.009), ('hillary_clinton', 0.008), ('voter', 0.008), ('party', 0.007), ('people', 0.007), ('vote', 0.007), ('state', 0.006), ('president', 0.006), ('cruz', 0.005), ('democrat', 0.005), ('would', 0.005), ('poll', 0.005), ('think', 0.005)]\n",
      "\n",
      "Topic #3:\n",
      "[('trump', 0.036), ('president', 0.02), ('obama', 0.012), ('would', 0.009), ('white_house', 0.009), ('russia', 0.009), ('country', 0.007), ('administration', 0.006), ('american', 0.006), ('mr', 0.006), ('russian', 0.006), ('leader', 0.005), ('also', 0.005), ('united_state', 0.005), ('donald_trump', 0.005), ('official', 0.004), ('former', 0.004), ('meeting', 0.004), ('government', 0.004), ('deal', 0.004)]\n",
      "\n",
      "Topic #4:\n",
      "[('attack', 0.011), ('people', 0.01), ('isi', 0.01), ('country', 0.01), ('government', 0.007), ('group', 0.006), ('muslim', 0.006), ('syria', 0.005), ('also', 0.005), ('force', 0.004), ('killed', 0.004), ('military', 0.004), ('city', 0.004), ('war', 0.004), ('many', 0.004), ('official', 0.003), ('year', 0.003), ('security', 0.003), ('turkey', 0.003), ('two', 0.003)]\n",
      "\n",
      "Topic #5:\n",
      "[('police', 0.014), ('city', 0.009), ('family', 0.008), ('two', 0.007), ('officer', 0.007), ('people', 0.006), ('home', 0.006), ('day', 0.006), ('man', 0.005), ('child', 0.005), ('according', 0.005), ('told', 0.004), ('three', 0.004), ('shooting', 0.004), ('cnn', 0.004), ('time', 0.004), ('car', 0.004), ('state', 0.004), ('student', 0.003), ('death', 0.003)]\n",
      "\n",
      "Topic #6:\n",
      "[('would', 0.011), ('state', 0.011), ('law', 0.009), ('republican', 0.007), ('american', 0.006), ('bill', 0.006), ('people', 0.006), ('policy', 0.005), ('president', 0.005), ('house', 0.004), ('right', 0.004), ('issue', 0.004), ('also', 0.004), ('plan', 0.004), ('could', 0.004), ('senate', 0.004), ('government', 0.004), ('federal', 0.004), ('gop', 0.004), ('court', 0.004)]\n",
      "\n",
      "Topic #7:\n",
      "[('investigation', 0.009), ('report', 0.008), ('official', 0.008), ('case', 0.007), ('information', 0.006), ('time', 0.006), ('email', 0.006), ('according', 0.006), ('fbi', 0.005), ('also', 0.005), ('statement', 0.005), ('told', 0.005), ('former', 0.004), ('comey', 0.004), ('evidence', 0.004), ('reported', 0.004), ('cnn', 0.004), ('intelligence', 0.004), ('would', 0.004), ('charge', 0.004)]\n",
      "\n",
      "Topic #8:\n",
      "[('woman', 0.01), ('show', 0.007), ('year', 0.005), ('also', 0.005), ('time', 0.005), ('black', 0.004), ('life', 0.004), ('new', 0.004), ('first', 0.003), ('like', 0.003), ('story', 0.003), ('white', 0.003), ('film', 0.003), ('wrote', 0.003), ('star', 0.003), ('book', 0.003), ('people', 0.003), ('movie', 0.003), ('made', 0.003), ('family', 0.002)]\n",
      "\n",
      "Topic #9:\n",
      "[('company', 0.017), ('year', 0.016), ('million', 0.015), ('business', 0.01), ('market', 0.009), ('percent', 0.009), ('according', 0.009), ('money', 0.008), ('price', 0.007), ('number', 0.006), ('billion', 0.006), ('job', 0.006), ('also', 0.006), ('new', 0.006), ('investor', 0.006), ('report', 0.005), ('cost', 0.005), ('study', 0.005), ('economy', 0.005), ('since', 0.005)]\n",
      "\n",
      "Topic #10:\n",
      "[('company', 0.012), ('new', 0.007), ('china', 0.006), ('apple', 0.006), ('also', 0.005), ('car', 0.005), ('tesla', 0.005), ('could', 0.005), ('google', 0.004), ('would', 0.004), ('technology', 0.004), ('system', 0.004), ('facebook', 0.003), ('north_korea', 0.003), ('ceo', 0.003), ('product', 0.003), ('use', 0.003), ('time', 0.003), ('chinese', 0.003), ('service', 0.003)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics_with_wts = [item[0] for item in topics_coherences]\n",
    "print('LDA Topics with Weights')\n",
    "print('='*50)\n",
    "for idx, topic in enumerate(topics_with_wts):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([(term, round(wt, 3)) for wt, term in topic])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics without Weights\n",
      "==================================================\n",
      "Topic #1:\n",
      "['people', 'like', 'get', 'say', 'would', 'think', 'thing', 'time', 'know', 'going', 'way', 'want', 'go', 'could', 'make', 'even', 'really', 'see', 'back', 'much']\n",
      "\n",
      "Topic #2:\n",
      "['trump', 'clinton', 'campaign', 'republican', 'donald_trump', 'say', 'election', 'candidate', 'hillary_clinton', 'voter', 'party', 'people', 'vote', 'state', 'president', 'cruz', 'democrat', 'would', 'poll', 'think']\n",
      "\n",
      "Topic #3:\n",
      "['trump', 'president', 'obama', 'would', 'white_house', 'russia', 'country', 'administration', 'american', 'mr', 'russian', 'leader', 'also', 'united_state', 'donald_trump', 'official', 'former', 'meeting', 'government', 'deal']\n",
      "\n",
      "Topic #4:\n",
      "['attack', 'people', 'isi', 'country', 'government', 'group', 'muslim', 'syria', 'also', 'force', 'killed', 'military', 'city', 'war', 'many', 'official', 'year', 'security', 'turkey', 'two']\n",
      "\n",
      "Topic #5:\n",
      "['police', 'city', 'family', 'two', 'officer', 'people', 'home', 'day', 'man', 'child', 'according', 'told', 'three', 'shooting', 'cnn', 'time', 'car', 'state', 'student', 'death']\n",
      "\n",
      "Topic #6:\n",
      "['would', 'state', 'law', 'republican', 'american', 'bill', 'people', 'policy', 'president', 'house', 'right', 'issue', 'also', 'plan', 'could', 'senate', 'government', 'federal', 'gop', 'court']\n",
      "\n",
      "Topic #7:\n",
      "['investigation', 'report', 'official', 'case', 'information', 'time', 'email', 'according', 'fbi', 'also', 'statement', 'told', 'former', 'comey', 'evidence', 'reported', 'cnn', 'intelligence', 'would', 'charge']\n",
      "\n",
      "Topic #8:\n",
      "['woman', 'show', 'year', 'also', 'time', 'black', 'life', 'new', 'first', 'like', 'story', 'white', 'film', 'wrote', 'star', 'book', 'people', 'movie', 'made', 'family']\n",
      "\n",
      "Topic #9:\n",
      "['company', 'year', 'million', 'business', 'market', 'percent', 'according', 'money', 'price', 'number', 'billion', 'job', 'also', 'new', 'investor', 'report', 'cost', 'study', 'economy', 'since']\n",
      "\n",
      "Topic #10:\n",
      "['company', 'new', 'china', 'apple', 'also', 'car', 'tesla', 'could', 'google', 'would', 'technology', 'system', 'facebook', 'north_korea', 'ceo', 'product', 'use', 'time', 'chinese', 'service']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('LDA Topics without Weights')\n",
    "print('='*50)\n",
    "for idx, topic in enumerate(topics_with_wts):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([term for wt, term in topic])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Coherence Score (Cv): 0.4611817097708893\n",
      "Avg. Coherence Score (UMass): -1.3628021129807835\n",
      "Model Perplexity: -8.525454085960877\n"
     ]
    }
   ],
   "source": [
    "cv_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus,\n",
    "                                                 texts=norm_corpus_bigrams,\n",
    "                                                     dictionary=dictionary,\n",
    "                                                      coherence='c_v')\n",
    "avg_coherence_cv = cv_coherence_model_lda.get_coherence()\n",
    "umass_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus,\n",
    "                                               texts=norm_corpus_bigrams,\n",
    "                                                     dictionary=dictionary,\n",
    "                                                        coherence='u_mass')\n",
    "avg_coherence_umass = umass_coherence_model_lda.get_coherence()\n",
    "perplexity = lda_model.log_perplexity(bow_corpus)\n",
    "print('Avg. Coherence Score (Cv):', avg_coherence_cv)\n",
    "print('Avg. Coherence Score (UMass):', avg_coherence_umass)\n",
    "print('Model Perplexity:', perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECKING TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "      <th>Topic 6</th>\n",
       "      <th>Topic 7</th>\n",
       "      <th>Topic 8</th>\n",
       "      <th>Topic 9</th>\n",
       "      <th>Topic 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Term1</th>\n",
       "      <td>people</td>\n",
       "      <td>trump</td>\n",
       "      <td>trump</td>\n",
       "      <td>attack</td>\n",
       "      <td>police</td>\n",
       "      <td>would</td>\n",
       "      <td>investigation</td>\n",
       "      <td>woman</td>\n",
       "      <td>company</td>\n",
       "      <td>company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term2</th>\n",
       "      <td>like</td>\n",
       "      <td>clinton</td>\n",
       "      <td>president</td>\n",
       "      <td>people</td>\n",
       "      <td>city</td>\n",
       "      <td>state</td>\n",
       "      <td>report</td>\n",
       "      <td>show</td>\n",
       "      <td>year</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term3</th>\n",
       "      <td>get</td>\n",
       "      <td>campaign</td>\n",
       "      <td>obama</td>\n",
       "      <td>isi</td>\n",
       "      <td>family</td>\n",
       "      <td>law</td>\n",
       "      <td>official</td>\n",
       "      <td>year</td>\n",
       "      <td>million</td>\n",
       "      <td>china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term4</th>\n",
       "      <td>say</td>\n",
       "      <td>republican</td>\n",
       "      <td>would</td>\n",
       "      <td>country</td>\n",
       "      <td>two</td>\n",
       "      <td>republican</td>\n",
       "      <td>case</td>\n",
       "      <td>also</td>\n",
       "      <td>business</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term5</th>\n",
       "      <td>would</td>\n",
       "      <td>donald_trump</td>\n",
       "      <td>white_house</td>\n",
       "      <td>government</td>\n",
       "      <td>officer</td>\n",
       "      <td>american</td>\n",
       "      <td>information</td>\n",
       "      <td>time</td>\n",
       "      <td>market</td>\n",
       "      <td>also</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term6</th>\n",
       "      <td>think</td>\n",
       "      <td>say</td>\n",
       "      <td>russia</td>\n",
       "      <td>group</td>\n",
       "      <td>people</td>\n",
       "      <td>bill</td>\n",
       "      <td>time</td>\n",
       "      <td>black</td>\n",
       "      <td>percent</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term7</th>\n",
       "      <td>thing</td>\n",
       "      <td>election</td>\n",
       "      <td>country</td>\n",
       "      <td>muslim</td>\n",
       "      <td>home</td>\n",
       "      <td>people</td>\n",
       "      <td>email</td>\n",
       "      <td>life</td>\n",
       "      <td>according</td>\n",
       "      <td>tesla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term8</th>\n",
       "      <td>time</td>\n",
       "      <td>candidate</td>\n",
       "      <td>administration</td>\n",
       "      <td>syria</td>\n",
       "      <td>day</td>\n",
       "      <td>policy</td>\n",
       "      <td>according</td>\n",
       "      <td>new</td>\n",
       "      <td>money</td>\n",
       "      <td>could</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term9</th>\n",
       "      <td>know</td>\n",
       "      <td>hillary_clinton</td>\n",
       "      <td>american</td>\n",
       "      <td>also</td>\n",
       "      <td>man</td>\n",
       "      <td>president</td>\n",
       "      <td>fbi</td>\n",
       "      <td>first</td>\n",
       "      <td>price</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term10</th>\n",
       "      <td>going</td>\n",
       "      <td>voter</td>\n",
       "      <td>mr</td>\n",
       "      <td>force</td>\n",
       "      <td>child</td>\n",
       "      <td>house</td>\n",
       "      <td>also</td>\n",
       "      <td>like</td>\n",
       "      <td>number</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term11</th>\n",
       "      <td>way</td>\n",
       "      <td>party</td>\n",
       "      <td>russian</td>\n",
       "      <td>killed</td>\n",
       "      <td>according</td>\n",
       "      <td>right</td>\n",
       "      <td>statement</td>\n",
       "      <td>story</td>\n",
       "      <td>billion</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term12</th>\n",
       "      <td>want</td>\n",
       "      <td>people</td>\n",
       "      <td>leader</td>\n",
       "      <td>military</td>\n",
       "      <td>told</td>\n",
       "      <td>issue</td>\n",
       "      <td>told</td>\n",
       "      <td>white</td>\n",
       "      <td>job</td>\n",
       "      <td>system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term13</th>\n",
       "      <td>go</td>\n",
       "      <td>vote</td>\n",
       "      <td>also</td>\n",
       "      <td>city</td>\n",
       "      <td>three</td>\n",
       "      <td>also</td>\n",
       "      <td>former</td>\n",
       "      <td>film</td>\n",
       "      <td>also</td>\n",
       "      <td>facebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term14</th>\n",
       "      <td>could</td>\n",
       "      <td>state</td>\n",
       "      <td>united_state</td>\n",
       "      <td>war</td>\n",
       "      <td>shooting</td>\n",
       "      <td>plan</td>\n",
       "      <td>comey</td>\n",
       "      <td>wrote</td>\n",
       "      <td>new</td>\n",
       "      <td>north_korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term15</th>\n",
       "      <td>make</td>\n",
       "      <td>president</td>\n",
       "      <td>donald_trump</td>\n",
       "      <td>many</td>\n",
       "      <td>cnn</td>\n",
       "      <td>could</td>\n",
       "      <td>evidence</td>\n",
       "      <td>star</td>\n",
       "      <td>investor</td>\n",
       "      <td>ceo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term16</th>\n",
       "      <td>even</td>\n",
       "      <td>cruz</td>\n",
       "      <td>official</td>\n",
       "      <td>official</td>\n",
       "      <td>time</td>\n",
       "      <td>senate</td>\n",
       "      <td>reported</td>\n",
       "      <td>book</td>\n",
       "      <td>report</td>\n",
       "      <td>product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term17</th>\n",
       "      <td>really</td>\n",
       "      <td>democrat</td>\n",
       "      <td>former</td>\n",
       "      <td>year</td>\n",
       "      <td>car</td>\n",
       "      <td>government</td>\n",
       "      <td>cnn</td>\n",
       "      <td>people</td>\n",
       "      <td>cost</td>\n",
       "      <td>use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term18</th>\n",
       "      <td>see</td>\n",
       "      <td>would</td>\n",
       "      <td>meeting</td>\n",
       "      <td>security</td>\n",
       "      <td>state</td>\n",
       "      <td>federal</td>\n",
       "      <td>intelligence</td>\n",
       "      <td>movie</td>\n",
       "      <td>study</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term19</th>\n",
       "      <td>back</td>\n",
       "      <td>poll</td>\n",
       "      <td>government</td>\n",
       "      <td>turkey</td>\n",
       "      <td>student</td>\n",
       "      <td>gop</td>\n",
       "      <td>would</td>\n",
       "      <td>made</td>\n",
       "      <td>economy</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term20</th>\n",
       "      <td>much</td>\n",
       "      <td>think</td>\n",
       "      <td>deal</td>\n",
       "      <td>two</td>\n",
       "      <td>death</td>\n",
       "      <td>court</td>\n",
       "      <td>charge</td>\n",
       "      <td>family</td>\n",
       "      <td>since</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Topic 1          Topic 2         Topic 3     Topic 4    Topic 5  \\\n",
       "Term1   people            trump           trump      attack     police   \n",
       "Term2     like          clinton       president      people       city   \n",
       "Term3      get         campaign           obama         isi     family   \n",
       "Term4      say       republican           would     country        two   \n",
       "Term5    would     donald_trump     white_house  government    officer   \n",
       "Term6    think              say          russia       group     people   \n",
       "Term7    thing         election         country      muslim       home   \n",
       "Term8     time        candidate  administration       syria        day   \n",
       "Term9     know  hillary_clinton        american        also        man   \n",
       "Term10   going            voter              mr       force      child   \n",
       "Term11     way            party         russian      killed  according   \n",
       "Term12    want           people          leader    military       told   \n",
       "Term13      go             vote            also        city      three   \n",
       "Term14   could            state    united_state         war   shooting   \n",
       "Term15    make        president    donald_trump        many        cnn   \n",
       "Term16    even             cruz        official    official       time   \n",
       "Term17  really         democrat          former        year        car   \n",
       "Term18     see            would         meeting    security      state   \n",
       "Term19    back             poll      government      turkey    student   \n",
       "Term20    much            think            deal         two      death   \n",
       "\n",
       "           Topic 6        Topic 7 Topic 8    Topic 9     Topic 10  \n",
       "Term1        would  investigation   woman    company      company  \n",
       "Term2        state         report    show       year          new  \n",
       "Term3          law       official    year    million        china  \n",
       "Term4   republican           case    also   business        apple  \n",
       "Term5     american    information    time     market         also  \n",
       "Term6         bill           time   black    percent          car  \n",
       "Term7       people          email    life  according        tesla  \n",
       "Term8       policy      according     new      money        could  \n",
       "Term9    president            fbi   first      price       google  \n",
       "Term10       house           also    like     number        would  \n",
       "Term11       right      statement   story    billion   technology  \n",
       "Term12       issue           told   white        job       system  \n",
       "Term13        also         former    film       also     facebook  \n",
       "Term14        plan          comey   wrote        new  north_korea  \n",
       "Term15       could       evidence    star   investor          ceo  \n",
       "Term16      senate       reported    book     report      product  \n",
       "Term17  government            cnn  people       cost          use  \n",
       "Term18     federal   intelligence   movie      study         time  \n",
       "Term19         gop          would    made    economy      chinese  \n",
       "Term20       court         charge  family      since      service  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_df = pd.DataFrame([[term for wt, term in topic]\n",
    "                              for topic in topics_with_wts],\n",
    "                         columns = ['Term'+str(i) for i in range(1, 21)],\n",
    "                         index=['Topic '+str(t) for t in range(1, lda_model.num_topics+1)]).T\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Terms per Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>people, like, get, say, would, think, thing, time, know, going, way, want, go, could, make, even, really, see, back, much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>trump, clinton, campaign, republican, donald_trump, say, election, candidate, hillary_clinton, voter, party, people, vote, state, president, cruz, democrat, would, poll, think</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>trump, president, obama, would, white_house, russia, country, administration, american, mr, russian, leader, also, united_state, donald_trump, official, former, meeting, government, deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic4</th>\n",
       "      <td>attack, people, isi, country, government, group, muslim, syria, also, force, killed, military, city, war, many, official, year, security, turkey, two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic5</th>\n",
       "      <td>police, city, family, two, officer, people, home, day, man, child, according, told, three, shooting, cnn, time, car, state, student, death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic6</th>\n",
       "      <td>would, state, law, republican, american, bill, people, policy, president, house, right, issue, also, plan, could, senate, government, federal, gop, court</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic7</th>\n",
       "      <td>investigation, report, official, case, information, time, email, according, fbi, also, statement, told, former, comey, evidence, reported, cnn, intelligence, would, charge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic8</th>\n",
       "      <td>woman, show, year, also, time, black, life, new, first, like, story, white, film, wrote, star, book, people, movie, made, family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic9</th>\n",
       "      <td>company, year, million, business, market, percent, according, money, price, number, billion, job, also, new, investor, report, cost, study, economy, since</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic10</th>\n",
       "      <td>company, new, china, apple, also, car, tesla, could, google, would, technology, system, facebook, north_korea, ceo, product, use, time, chinese, service</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                    Terms per Topic\n",
       "Topic1   people, like, get, say, would, think, thing, time, know, going, way, want, go, could, make, even, really, see, back, much                                                                 \n",
       "Topic2   trump, clinton, campaign, republican, donald_trump, say, election, candidate, hillary_clinton, voter, party, people, vote, state, president, cruz, democrat, would, poll, think           \n",
       "Topic3   trump, president, obama, would, white_house, russia, country, administration, american, mr, russian, leader, also, united_state, donald_trump, official, former, meeting, government, deal\n",
       "Topic4   attack, people, isi, country, government, group, muslim, syria, also, force, killed, military, city, war, many, official, year, security, turkey, two                                     \n",
       "Topic5   police, city, family, two, officer, people, home, day, man, child, according, told, three, shooting, cnn, time, car, state, student, death                                                \n",
       "Topic6   would, state, law, republican, american, bill, people, policy, president, house, right, issue, also, plan, could, senate, government, federal, gop, court                                 \n",
       "Topic7   investigation, report, official, case, information, time, email, according, fbi, also, statement, told, former, comey, evidence, reported, cnn, intelligence, would, charge               \n",
       "Topic8   woman, show, year, also, time, black, life, new, first, like, story, white, film, wrote, star, book, people, movie, made, family                                                          \n",
       "Topic9   company, year, million, business, market, percent, according, money, price, number, billion, job, also, new, investor, report, cost, study, economy, since                                \n",
       "Topic10  company, new, china, apple, also, car, tesla, could, google, would, technology, system, facebook, north_korea, ceo, product, use, time, chinese, service                                  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "topics_df = pd.DataFrame([', '.join([term for wt, term in topic])\n",
    "                              for topic in topics_with_wts],\n",
    "                         columns = ['Terms per Topic'],\n",
    "                         index=['Topic'+str(t) for t in range(1, lda_model.num_topics+1)]\n",
    "                         )\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTERPRETING RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_results = lda_model[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_topics = [sorted(topics, key=lambda record: -record[1])[0]\n",
    "                     for topics in tm_results]\n",
    "corpus_topics[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_topic_df = pd.DataFrame()\n",
    "corpus_topic_df['Document'] = range(0, len(papers) - 67)\n",
    "corpus_topic_df['Dominant Topic'] = [item[0]+1 for item in corpus_topics]\n",
    "corpus_topic_df['Contribution %'] = [round(item[1]*100, 2) for item in corpus_topics]\n",
    "corpus_topic_df['Topic Desc'] = [topics_df.iloc[t[0]]['Terms per Topic'] for t in corpus_topics]\n",
    "corpus_topic_df['Title'] = pre_titles\n",
    "corpus_topic_df['Paper'] = pre_papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "topic_stats_df = corpus_topic_df.groupby('Dominant Topic').agg({\n",
    "                                                'Dominant Topic': {\n",
    "                                                    'Doc Count': np.size,\n",
    "                                                    '% Total Docs': np.size }\n",
    "                                              })\n",
    "topic_stats_df = topic_stats_df['Dominant Topic'].reset_index()\n",
    "topic_stats_df['% Total Docs'] = topic_stats_df['% Total Docs'].apply(lambda row: round((row*100) / len(papers), 2))\n",
    "topic_stats_df['Topic Desc'] = [topics_df.iloc[t]['Terms per Topic'] for t in range(len(topic_stats_df))]\n",
    "topic_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### document most dominant topic with highest contribution %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_topic_df.sort_values(by='Contribution %', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "(corpus_topic_df[corpus_topic_df['Document']\n",
    "                 .isin([681, 9, 392, 1622, 17,\n",
    "                        906, 996, 503, 13, 733])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_topic_df.groupby('Dominant Topic').apply(lambda topic_set:\n",
    "                                            (topic_set.sort_values(by=['Contribution %'],\n",
    "                                                   ascending=False).iloc[0]))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
